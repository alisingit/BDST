## Описание:

Практическая часть курса состоит из 3 лабораторных работ. Каждая  
лабораторная работа направлена на получение практических навыков и теоретических  
знаний связанных с хранением и обработкой данных. Результатом выполнения всех  
лабораторных работ, должна быть система, отвечающая за сбор, обработку, хранение и  
анализ полученных данных.  
Лабораторные работы выполняются в команде до 2 человек, возможно  
индивидуальное выполнение. Проекты должны быть сохранены в GitHub, ссылку на  
репозиторий прикрепить в таблице.

## Допустимые источники данных:

В качестве источников данных должны быть использованы открытые источники:  
веб\-сайты, API. Ниже представлен список возможных тематик, но также допустимо  
предложить свою.  
Перед началом работы с “кастомным” источником, его необходимо защитить.  
Защита заключается в устном/графическом/текстовом представлении темы,  
используемых источников и плана дальнейшего анализа данных.  
Источники не должны совпадать у различных команд. Для сверки какие  
источники уже взяты, воспользуйтесь таблицей источников(URL).

## Примеры:

* Анализ социальных сетей, форумов: T-банк Пульс, Reddit, Пикабу;  
* Новостные агрегаторы: РБК, Forbes, Телеграмм каналы;  
* Инвестиции: Любые агрегаторы инвестиционных индикаторов;  
* Геоданные;  
* Онлайн табло аэропортов и вокзалов СПБ;  
* Агрегаторы продажи билетов;  
* Онлайн торговые площадки: Ozon, Wildberries, Amazon.  
* Онлайн игры: игровые торговые площадки, faceit, steam.

*Выбор “экзотических” источников данных будет поощряться*  
*дополнительными баллами. Возможен альтернативный трек обработки данных для*  
*подобных источников (ЛР2).*

# Лабораторная работа №1 “Работа с Airflow. ETL-процесс.”:

В рамках данной работы вам необходимо реализовать ETL процесс, отвечающий  
за сбор и загрузку сырых данных в хранилище (слой ODS). База данных должна быть  
выбрана командой, выбор необходимо аргументировать. Оркестрация ETL процессов  
должна быть реализована с помощью Apache Airflow (https://airflow.apache.org/).

### Этапы выполнения:

1\. Развернуть сервис Airflow в Docker-контейнере, используя docker-compose  
конфигурацию (примеры можно найти в официальной документации,  
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.ht  
ml);  
2\. Выбрать 3 различных сервиса для хранения данных. Например: s3, mongodb,  
oracle. Добавить конфигурацию для развертывания хранилищ в docker-compose.  
3\. Реализовать не менее 3 различных ETL процессов (DAGов). При нехватке  
данных на одной платформе, данные можно брать с нескольких. Например: ozon  
\+ wildberries, aviasales \+ tutu.ru;  
*Данные можно разделять логически в рамках одного источника: комментарии,*  
*товары, отзывы.*  
4\. В результате работы ETL процессов данные должны быть выгружены в  
выбранные базы данных;  
5\. Провести сравнительный анализ выбранных хранилищ данных. Сравнительные  
критерии необходимо выбрать самостоятельно. Выбрать наиболее подходящее  
хранилище для полученных данных.  
Для защиты необходимо предоставить отчет, описывающий этапы выполнения  
работы, а также исходный код ETL процессов и docker-compose файл.  
Обязательным условием является демонстрация работы: веб интерфейс Airflow,  
выгруженные данные в базах данных, сравнительный анализ в виде графиков и/или  
Таблиц.

# Лабораторная работа №2 “Нормализация данных. Data Quality.”:

Разработка базового аналитического хранилища данных на основе сырых  
данных из ЛР1. Формирование процессов очистки, трансформации и загрузки данных в  
слой DDS.

### Этапы выполнения:

1\. Определить структуру хранилища: схема "звезда" или "снежинка". Привести  
данные к 3НФ.  
Пример сущностей для DDS:  
Факты: продажи, комментарии, активность пользователей,  
Измерения: товары, пользователи, даты, категории;  
В некоторых источниках данных может возникнуть проблема с выбором  
подходящей сущности для фактов. При возникновении такой ситуации  
достаточно нормализовать данные.  
2\. Создать новые DAG в Airflow для трансформации данных:  
DDS-слой: Скрипты очистки (удаление дубликатов, приведение типов),  
обогащение, агрегация;  
3\. Где возможно, сущности должны соответствовать концепции медленно  
изменяющихся измерений (SCD), чтобы изменения значений атрибутов  
сущностей могли отслеживаться во времени;  
4\. Построение зависимостей между ETL-процессами. Процессы детального слоя  
должны ожидать завершения соответствующих расчетов исходных данных.  
Для реализации зависимостей возможно использовать сенсоры  
(https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html).  
Альтернативные варианты приветствуются.  
5\. Data Quality: реализовать DAG для проверки качества данных.  
Примеры возможных проверок:  
Сравнить объемы данных до/после трансформации,  
Проверить отсутствие аномальных значений в атрибутах.  
Для защиты необходимо предоставить отчет, описывающий этапы выполнения  
работы, а также исходный код SQL скриптов и ETL процессов.  
Быть готовым продемонстрировать работу и веб интерфейс Airflow с  
полученными ETL процессами, а также выгруженные данные в базе данных.

# Лабораторная работа №3 “Построение витрин данных. Визуализация данных с помощью Superset.”:

Цель данной лабораторной работы является построение витрин данных, а также  
визуализация полученных метрик с помощью Superset. Для успешного защиты  
необходимо сформировать витрины данных. Каждая витрина должна обеспечивать  
возможность получения конкретных аналитических выводов. Например,  
продемонстрировать наиболее популярные тематики, обсуждаемые пользователями  
форума, или пики пользовательской активности платформы.

### Этапы выполнения:

1\. Развертывание сервиса Superset необходимо добавить в существующую  
конфигурацию docker-compose;  
2\. Сформировать витрины данных на основе детального слоя данных.  
3\. Реализовать скрипт формирования данных витрин и обернуть данный скрипт в  
ETL процесс. Аналогично связать данный процесс с ETL процессами  
детального слоя.  
4\. На основе полученных данных сформировать дашборд из не менее 5 различных  
визуализаций. Аргументировать выбор визуализаций.  
5\. Сделать вывод на основе полученных данных и визуализаций.  
6\. Для защиты необходимо предоставить отчет, описывающий этапы выполнения  
работы, а также исходный код SQL скриптов и ETL процессов.  
Быть готовым продемонстрировать работу и веб интерфейс Airflow с  
полученными ETL процессами, визуализациями, а также выгруженные данные в базе  
данных.  
